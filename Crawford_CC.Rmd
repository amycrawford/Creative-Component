---
title: \Large __Performance of 2-class Classifiers on Data for which Labels are Missing by a Non-Random Mechanism__
author: "Amy Crawford"
date: "Spring 2017"
output:
  pdf_document: default
bibliography: ccbib.bib
header-includes:
- \usepackage{setspace}
- \doublespacing
- \usepackage{bigints}
fontsize: 12pt
---
\newcommand\ddfrac[2]{\frac{\displaystyle #1}{\displaystyle #2}} 

```{r, echo = F, message = F, warning = F}
library(tidyverse)
library(randomForest)
library(caret)
library(e1071)
library(sampling)
set.seed(42)
options(scipen = 999)

# Define train_values_url
train_values_url <- "http://s3.amazonaws.com/drivendata/data/7/public/4910797b-ee55-40a7-8668-10efd5c1b960.csv"

# Import train_values
train_values <- read.csv(train_values_url)  #write.csv(train_values, "./data/train_values.csv")

# Define train_labels_url
train_labels_url <- "http://s3.amazonaws.com/drivendata/data/7/public/0bf8bc6e-30d0-4c50-956a-603fc693d966.csv"

# Import train_labels
train_labels <- read.csv(train_labels_url)    # write.csv(train_labels, "./data/train_labels.csv")

train <- merge(train_labels, train_values, by = "id")      # write.csv(train, "./data/train.csv")

# combine labels for 2class classification problem
train$status_group <- factor(train$status_group, levels = c(levels(train$status_group), "requires attention"))
train$status_group[train$status_group == "functional needs repair"] <- "requires attention"
train$status_group[train$status_group == "non functional"] <- "requires attention"
train$status_group <- droplevels(train$status_group)

# data is the subset of train without any missing population information
data <- train[train$population != 0 & train$construction_year != 0,]


# Change date_recorded from a factor to a date
data$date_recorded <- as.Date(data$date_recorded)

# Change region code and district code from integers to factors
data$region_code <- factor(data$region_code)
data$district_code <- factor(data$district_code)

# interpolate missing construction year data
# data$construction_year[data$construction_year < 0] <- median(data$construction_year[data$construction_year > 0])

# Interpolate missing gps_height values
data$gps_height[data$gps_height == 0] <- median(data$gps_height[data$gps_height>0])
```

\vspace{1cm}

## Abstract  
When companies implement algorithms to expedite existing job functions it often leaves them with newly available employees, time, and money to investigate other unexplored parts of their market. To be as efficient as possible in an expansion such as this, the company can again turn to some machine learning algorithm to point to where the newly available resources should be focused in the market. Unfortunately, the only available data to train an algorithm on would then be data from the portion of the market that has already been explored by the company. In other words, whether a case in the data set would have a label or not is due to a specific non-random mechanism. I evaluate the performance of logistic regression, \textbf{s}upport \textbf{v}ector \textbf{m}achine (SVM), and random forest models under this framework. Additionally, I implement one classification method called \textbf{a}ctive \textbf{s}et \textbf{s}election \textbf{c}lassification (ASSC) that involves weighted bootstrap and emsemble learning.

\pagebreak 

## Introduction
### Overview of the Product
$\quad$ A large company manufactures small and large equipment for both personal and commercial use. I will call it Company X. Founded by one individual in the 1800s, this Fortune 100 company has grown to serve over 100 countries. The company has a strong distribution channel and relies heavily on its many dealerships to connect to consumers.  
  

$\quad$ A financial division of Company X provides financing options for equipment and industry inputs such as fuel, seeds, and fertilizers. Financing options are provided to all types of consumers including individuals, companies, and government entities. The consumer can choose to finance with an installment loan, a lease, or a revolving credit product. My project with the company focused on the revolving credit product. This product functions like a credit card and is used exclusively for industry inputs. Account information is held at the consumer's home dealership and the consumer can puchase industry inputs using the revolving credit product financed by Company X directly through the dealership.  
  
  
$\quad$ Unfortunately, there are consumers and dealers who misuse the revolving credit product. The financial division of the company is responsible for detecting and handling these dishonest individuals. One type of the product misuse comes in the form of the consumer attempting to obtain a cash advance. In this scenario, the consumer and dealer (usually) work together to submit a transaction that overstates the price or amount of industry input purchased. For example, the consumer's equipment might only hold 200 gallons of fuel but the dealer submits a transaction for 500 gallons of fuel on the consumer's revolving credit product account. This is particularly unsavory for Company X because the payment plans for the revolving credit product are necessarily very flexible. The consumers who use this financing option must often spend large sums of money on inputs to produce their product, but don't have the cash flow to repay Company X until their product is mature enough to sell. Because of the nature of this industry the revolving credit product has a payment schedule that requires large payments from the consumer during the season of high cash flow and much smaller payments from the consumer during seasons of low cash flow. Thus, when the consumer misuses the revolving credit product with the intention of receiving a cash advance, the company takes on a sizeable amount of unnecessary risk, usually during a season of low cash flow. 

$\quad$ Another type of misuse occurs when the consumer is indebted to the dealer and the dealer shifts the debt (and risk) to Company X by way of submitting a transaction on the revolving credit product of the consumer. When a transaction is placed with the intent of misusing the revolving credit product, it is labeled as a fraudulent transaction. Since account information for this revolving product is held within a dealership, Company X experiences little of what the reader might immediately think of as traditional credit card fraud. Nevertheless, any misuse of this revolving credit product is considered to be an attempt to defraud the company and is treated as such.  
  

### Overview of the Problem

$\quad$ When I joined Company X as an intern in May 2016, the financial division had resources to audit a small subset of transactions made in the United States on the revolving credit product. The group in charge of this task, which we will refer to as the audit group, was auditing all transactions that they considered to be "high risk". Any transaction that was audited by the group was subsequently labeled according to whether it was found to be fraudulent or not. 

$\quad$ While all transactions under consideration had large purchase amounts (> \$3,000), a transaction was considered to be of "high risk" to the company if it was made on an account with a high existing balance (> \$10,000) that was in a state of non-payment (> 60 days past payment due date). These account thresholds were chosen strategically because it was beleived that a higher rate of fraud existed among these "high risk" transactions. Additionally, the company stood to incur significant losses if transactions of this nature were found to be fraudulent. My first task as an intern was to use data available from past audits and build a model to rank incoming sets of "high risk" transactions in order of probability of fraud. Consequently, the audit group would be able to audit transactions in the order of the ranked list, and catch almost all of the fraudulent transactions in a shorter period.  

$\quad$ With implementation of the model built to rank the "high risk" transactions, the audit group anticipated having newly available time and resources. With these resources, the audit team wanted to expand to auditing a wider range of transactions. Thus, my second task was to rank all incoming transactions with large purchase amounts (> \$3,000), not just those that were considered "high risk", in order of probability of fraud. This more general pool of transactions were charged on accounts with any amount of existing balance that were not necessarily in a state of non-payment. This second task poses many challenges and leads to the problem of interest for this project. That is, how can we execute a 2-class classification or estimate fraud probability for transactions in the entire sample space if we only have labels for the "high risk" transactions, since those were the only transactions audited?  

$\quad$ When trying to build a classifier for the rest of the transactions we find ourselves in a situation where data cases are missing labels not at random, but by a very specific covariate-dependent mechanism. While some may simply view this as a severe case of extrapolation, it is a real scenario that arises in business settings when companies aquire additional resources to expand their operations. The ability to identify the next set of transactions to spend resources on is extremely valueable to Company X. Even a classifier that we know will perform relatively poorly can save a great deal of money in a situation such as this.

## Data

### Company X Data

$\quad$ From this point forward refer to data cases that are labeled as belonging to the training set. Similarly, data cases that need to be classified (those that are not labeled) will belong to the testing set. Since the mechanism dictating which data cases are labeled enforces strict thresholds on two variables (account balance > \$10,000 and state of non-payment > 60 days past due) we can think of the cases in the training set to be confined to a "corner" of the feature space. See Figure 1 for a simplified graphical representation of this in 2-dimensions.   


$\quad$ The goal of this project is to investigate the best way to conduct 2-class classification of data cases that lie outside of a corner of a feature space where a classifier is built. The Company X dataset poses two major concerns. The first is that since fraud occurs at such a low rate, the data labels (in the entire data set) are not at all balanced. The second concern comes with the assumption that a higher rate of fraud exists in the training set ("high risk" transactions) than the testing set. This suggests that the distribution of fraud in the training set differs from that in the testing set.  


\vspace{10mm}
  
```{r, echo = F, warning=F, message=F}
library(ggplot2)

xx <-   c(5.961082, 5.248245, 4.708966, 5.671326, 4.433232, 4.357607, 5.167873, 4.853946, 5.143228, 4.709360, 6.271040, 5.252045, 4.980793, 4.434479, 5.798951, 5.923462, 4.153027, 5.259458, 5.719942, 5.948664, 4.896080, 5.385232, 6.432585, 5.127687, 4.678420, 6.118465, 5.096537, 4.219662, 4.495718, 5.471352, 6.290242, 4.387649, 4.415240, 4.905839, 4.380557, 4.258602, 6.453856, 5.902553, 4.954064, 5.029280, 4.669923, 5.426856, 5.711694, 4.416741, 5.158100, 4.128384, 6.322056, 4.929333, 6.032343, 5.756474, 5.7, 6.1, 6.3)

yy <-   c(6.664424, 4.475116, 5.058735, 4.677676, 6.755465, 5.49342, 3.421063, 6.196025, 5.097960, 4.769714, 4.257160, 4.986530, 4.387976, 5.686114, 4.466184, 5.837568, 4.606358, 5.103151, 6.205043, 6.477158, 3.334588, 4.768281, 3.873592, 4.508289, 4.794013, 5.491599, 3.954841, 4.974506, 3.490423, 3.493849, 5.685999, 5.122628, 4.784721, 5.162455, 3.955719, 4.362908, 3.391666, 5.760552, 5.839150, 4.364777, 5.317820, 5.287228, 6.721844, 5.230617, 5.676528, 4.754410, 3.196681, 6.888295, 5.116039, 5.619979, 4, 3.5, 6.15)

df <- data.frame(xx, yy) 
df$shape <- ifelse(xx > 5.35 & yy > 5, "Labeled Data: Training Set", "Unlabeled Data: Testing Set")

ggplot(data = df) + geom_point(aes(x = xx, y = yy, shape = shape)) +
  geom_segment(aes(x = 4, xend = 6.6, y = 3, yend = 3), lty = 1) +
  geom_segment(aes(x = 4, xend = 4, y = 3, yend = 7), lty = 1) +
  geom_segment(aes(x = 5.35, xend = 6.6, y = 5, yend = 5), lty = 2) +
  geom_segment(aes(x = 5.35, xend = 5.35, y = 5, yend = 7), lty = 2) +
  theme_replace() +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) + 
  xlab("Variable 1") +
  ylab("Variable 2") + 
  scale_shape_manual(values = c(1,2)) + 
  labs(shape = "Legend") + 
  ggtitle("Figure 1")
```

### Introducing the Water Pump Data  

$\quad$ In the interest of complete demonstration of the methods in the remainder of the paper, the data used will not be the proprietary data from Company X that motivated the project. To allow for proper evaluation of the prediction methods, I proceed using data from a data science competition [@DrivenData]. The competition is a 3-class classification problem designed as an excercise for learning and exploration for intermediate level participants. Data were collected on 59,400 water pumps and their corresponding water points, in Tanzania. The pumps are used to draw water from community water points. The primary task of the posted competition is to predict which pumps are functional, which need minor repairs, and which don't work at all. Each pump in the data set is labeled according to these categories.      

$\quad$ The water pump data can be configured to be analogous to the motivating problem from Company X. The water pump data includes information on the year in which the water point was manufactured and the population density around the water point. These are two variables that I will build a "corner" on. Just as the employees at Company X were able to spend resources to label only "high risk" transactions on the revolving credit product, we can consider a similar scenario with the water pump data.

$\quad$ Hypothetically, consider that until very recently the organization responsible for monitoring the status of the water points and pumps had restricted resources of money, time, and employees and could only monitor old wells (built before 1985) that have a relatively large surrounding population (> 200 people). These water points are analogous to the "high risk" set of transactions from Company X. So, the organization has data on features of all pumps, but only pumps on these currently monitored water points are labeled as `functional`, `functional needs repair`, or `non functional`.  
  
  
$\quad$ Now, suppose the organization has acquired resources to hire a few new employees who will devote their time to monitoring and managing water points with less densely populated surrounding areas. The organization needs to use these few employees in a very strategic way. There are many, many wells that will fall under this group's jurisdiction and they need to target pumps that are likely broken, or need repairs. The organization doesn't want to waste time sending these employees all over the country to check on pumps that are functional. Using what we know about the status of the pumps that have been monitored (the "high risk" pumps), we need to predict which of the relatively new pumps that are located in less densely populated areas will require the attention of the new employees (i.e. the pumps that need repairs, or are broken).


### Structural Changes to the Data

$\quad$ The water pump data has 59,400 rows and 41 columns and contains information on various features of the pumps as well as the water points where they reside. Water pumps were originally labeled as `functional`, `functional needs repair`, and `non functional`. First, to parallel the motivating problem from Company X, we will turn this 3-class classification problem into a 2-class classification problem. To do this, consider the pumps labeled as `functional needs repair` and `non functional` together in one group labeled `requires attention`.

$\quad$ There is a variable in the data set, `population`, that provides information about the size of the population surrounding the water point. Nearly 36\% of the values in this column are zero. It is clear that many of the zeros simply indicate a missing value, but we acknowledge the possibility that there truly are not any residents residing in the vacinity of a water point. If the latter is the case, we can eliminate the water point from the data set since we do not wish to expend effort classifying a water point that is abandoned. If the former is true, then the zero represents a missing value. After looking for patterns that might help us understand these many missing values, we find that the location of the water point seems relevant. 

$\quad$ Tanzania is divided into regions and most zero values of the `population` variable are for water points in six of these regions [@DrivenData]. Since the focus of this problem is not a spatial analysis and it would not be particularly helpful to classify abandoned water points, we can proceed only considering water points with non-zero values of the `population` variable. Similarly, we will elimate any wells that are missing information about construction year. This effectively reduces the number of rows from 59,400 to 37,344. The map in left panel of Figure 2 displays all 59,400 water points in the original data set. The map in the right panel of Figure 2 includes only the water points with valid `population` and `construction_year` data. Both maps are colored by the classification of the water pump status. Maps were generated using shape files from the GADM spatial database [@shapefile].


\vspace{1.5cm}

```{r, echo = F, message = F, warning = F, eval = F}
library(maptools)
library(gridExtra)
library(gtable)
library(grid)
#install.packages("devtools")
#devtools::install_github("baptiste/egg")
library(egg)

shape <- readShapeSpatial("/Users/amycrawford/Documents/Creative Component/Creative-Component/TZA_adm_shp/TZA_adm1.shp")

country <- shape %>% ggplot(aes(x = long, y = lat)) + geom_path(aes(group = group))

map1 <- country + geom_point(data = train[train$latitude != 0 & train$longitude != 0,],
                     aes(x = longitude, y = latitude, colour = status_group), size = 0.5) +
  ggtitle("Figure 2", subtitle = "All Water Points") +
  labs(x = "Longitude", y = "Latitude", colour = "Status Group") +
  theme(legend.position = "bottom")

map2 <- country + geom_point(data = data[data$latitude != 0 & data$longitude != 0,],
                     aes(x = longitude, y = latitude, colour = status_group), size = 0.5) +
  ggtitle("", subtitle = "\nExclude Water Points with Zero\nPopulation or Construction Year") +
  labs(x = "Longitude", y = "Latitude") + guides(colour=FALSE)


ggarrange(map1, map2, ncol=2)
```

\vspace{1cm}

### Building the "Corner"

$\quad$ Recall that in the motivating problem from Company X, there was assumed to be a higher rate of fraud among the training cases. In order to be consistent with this motivating problem consider choosing water points to be members of the "cornered" training set based upon threshold values of the variables `population` and `construction_year`. The `population` variable discussed above ranges from 1 to 30,500. Water points with larger surrounding populations have a higher proportion of `requires attention` pumps than water points located in areas with fewer people. Figure 3 shows this graphically with a vertical line placed at the population level of 200 people. There are a relatively small number of wells with a surrounding population of more than 1000 people, so the histogram is restricted to population values of 900 or less for presentation purposes. Now, the first requirement for membership in the training data set is a `population` larger than 200 people.

\vspace{1cm}

```{r, warning=F, message = F, echo = F, fig.height=3, fig.width=7}
###### Analyze population variable
ggplot(data) + geom_histogram(aes(x = population,fill = status_group), binwidth = 65) + 
  xlim(0,1000) + 
  ylim(0,6000) + 
  ggtitle("Figure 3", subtitle = "Truncated Histogram of the Population Variable") + 
  labs(x = "Population", y = "Count", fill = "Status Group") + 
  geom_segment(aes(x = 200, xend = 200, y = 0, yend = 6000), lty = 1) + 
  annotate("text", x = 200, y = 5000, hjust = -0.05, label = "Population = 200")
```



$\quad$ The variable `construction_year` is simply the year in which water point was constructed. The variable ranges from 1960 to 2013. Visually, it is clear from Figure 4 that water points built before 1985 have a higher ratio of pumps that `require attention` to those that are `functional` than pumps built after 1985 (and certainly higher than those built after 1990). Thus, the second requirement for membership in the training data set is a `construction_year` before 1985. 

\vspace{1cm}

```{r, warning=F, message = F, echo = F, fig.height=3, fig.width=7}
###### Analyze construction_year variable
ggplot(data) + geom_bar(aes(construction_year, fill = status_group)) + 
  xlim(1960, 2013) + 
  ggtitle("Figure 4", subtitle = "Histogram of the Construction Year Variable") + 
  labs(x = "Construction Year", y = "Count", fill = "Status Group") + 
  geom_segment(aes(x = 1985, xend = 1985, y = 0, yend = 2600), lty = 1) + 
  annotate("text", x = 1985, y = 2300, hjust = -.05, label = "Construction Year = 1985")

# We only have responses for old (1960 - 1980) pumps in highly populated areas
data$corner <- ifelse(data$population > 200 & data$construction_year < 1985, 1, 0)
```

\vspace{1cm}

$\quad$ Now, the data have been divided into the training and testing sets based on the thresholds for the `population` and `construction_year` variables prescribed above. 
The training data has 2,886 rows and the testing data has 34,458 rows. The distribution of the target variable `status_group` in each of these data sets is summarized in Table 1. 

\vspace{1cm}

```{r, message=F, warning=F, fig.height=.5, echo = F, fig.align="center", fig.width = 9}
ggplot() + ggtitle("Table 1", subtitle = "Distribution of the Status Group Variable in Training and Testing Data Sets                           ") + theme_minimal()
```

\begin{center}
\begin{tabular}{l l l l}
  & \multicolumn{2}{ c }{Status Group (row \%)} \\ \cline{2-3}
  Data Set & "functional" & "requires attention" \hspace{3mm} & \hspace{7mm} Total\\ \hline
  Training Data \hspace{5mm} & 939 (32.54\%) & 1,947 (67.46\%) & \hspace{7mm} 2,886 (100\%)\\
  Testing Data & 19,770 (57.37\%) & 14,688 (42.63\%) & \hspace{7mm} 34,458 (100\%)\\ 
\hline
\end{tabular}
\end{center}

\vspace{1cm}

$\quad$ Because broken pumps are not quite such a rare event as fraud on a revolving credit product, the Tanzania water pump data are not unbalanced like the data from Company X. We have, however, constructed a training set including only water points with `population` larger than 200 and `construction_year` before 1985 effectively forcing the training data into a "corner" of the feature space. The Company X data was believed to have a higher rate of fraud among the "cornered" training data cases than the testing data cases. Using the chosen thresholds to split the Tanzania water point data, we are able to mirror this phenomenon. Now, labels on the status of the water pumpts are removed from the testing dataset and we proceed to select features and model as if we never had them. They will be used only to evaluate model performance.

### Feature Selection

```{r, echo = F, message = F, warning = F}
#Not sure what num_private is, and it didn't improve the model.
data$num_private <- NULL

#Removed because only one unique value
data$recorded_by <- NULL

#Removed because not important to this problem
data$date_recorded <- NULL

#Removed because there were too many unique values
data$wpt_name <- NULL

#Removed because both are similar to extraction_type_class
data$extraction_type_group <- NULL
data$extraction_type <- NULL

#Removed because similar to payment
data$payment_type <- NULL

#Removed because similar to quality_group.
data$water_quality <- NULL

#Removed because similar to source_type (source = more specific version, source_class = less specific version)
source <- levels(data$source)
source_class <- levels(data$source_class)

data$source <- NULL
data$source_class <- NULL 


#Removed because all are LOCATION variables. Region should be sufficient to account for location.
data$district_code <- NULL
data$region <- NULL
data$region_code <- NULL
data$subvillage <- NULL
data$ward <- NULL
data$latitude <- NULL
data$longitude <- NULL

#Removed because similar to waterpoint_type
data$waterpoint_type_group <- NULL

#Removed because duplicate of quantity
data$quantity_group <- NULL

#Removed because too many unique values. Another option is to group the installers to reduce the number of unique values
data$installer <- NULL

# Removed because too many unique values.
data$funder <- NULL
data$scheme_name <- NULL
data$lga <- NULL

# Combine "None", "", and "Other" levels of scheme_management
data$management <- NULL
data[data$scheme_management %in% c("None", ""),]$scheme_management <- "Other"
data$scheme_management <- droplevels(data$scheme_management)

# Relabel missing permit entries
data$permit <- factor(data$permit, levels = c(levels(data$permit), "Unknown"))
data[data$permit %in% c(""),]$permit <- "Unknown"


# Relabel missing public_meeting entries
data$public_meeting<- factor(data$public_meeting, levels = c(levels(data$public_meeting), "Unknown"))
data[data$public_meeting %in% c(""),]$public_meeting <- "Unknown"

# get rid of id, corner, population, and construction_year columns
data$id <- NULL

#Separate data into train and test set
train <- data[data$corner == 1,]
test <- data[data$corner == 0,]

train$corner <- NULL
test$corner <- NULL

test_y <- test$status_group
test$status_group <- NULL


train_15 <- train
train_15$amount_tsh <- (train_15$amount_tsh - mean(train_15$amount_tsh))/sd(train_15$amount_tsh)
train_15$gps_height <- (train_15$gps_height - mean(train_15$gps_height))/sd(train_15$gps_height)
train_15$population <- (train_15$population - mean(train_15$population))/sd(train_15$population)
train_15$construction_year <- (train_15$construction_year -
                                 mean(train_15$construction_year))/sd(train_15$construction_year)
train_15 <- droplevels(train_15)
train_15 <- subset(train_15, select = c(2:3, 5, 9, 4, 6:8, 10:16, status_group))

train_13 <- train_15
train_13$population <- NULL
train_13$construction_year <- NULL
train_13 <- droplevels(train_13)


test_15 <- test
test_15[test_15$extraction_type_class %in% "rope pump",]$extraction_type_class <- "other"
test_15[test_15$waterpoint_type %in% "dam",]$waterpoint_type <- "other"
test_15$amount_tsh <- (test_15$amount_tsh - mean(test_15$amount_tsh))/sd(test_15$amount_tsh)
test_15$gps_height <- (test_15$gps_height - mean(test_15$gps_height))/sd(test_15$gps_height)
test_15$population <- (test_15$population - mean(test_15$population))/sd(test_15$population)
test_15$construction_year <- (test_15$construction_year -
                                 mean(test_15$construction_year))/sd(test_15$construction_year)
test_15 <- droplevels(test_15)
test_15 <- subset(test_15, select = c(1:2, 4, 8, 3, 5:7, 9:15))

test_13 <- test_15
test_13$population <- NULL
test_13$construction_year <- NULL
test_13 <- droplevels(test_13)
```

$\quad$ There are many groups of variables in the data that are meant to give the same information at different levels of detail. For example, the variables `source`, `source_type`, and `source_class` all provide information about the water source feeding the water point. These variables are all coded as factors. The `source_class` variable gives very general, high level information while `source_type` is a bit less general, giving a moderate amount of detail. The `source` variable is the most detailed of the three. Table 2 shows the unique values each of these variables can take. These variables are highly correlated and will clearly introduce multicollinearity during the modeling process. For variable groups like these, the one providing what we judge to be the most appropriate level of detail is chosen using visual comparison and numerical summaries. For the three variables discussed here, `source_type` was chosen as the most appropriate to inform the subsequent models about the water source feeding the water point. A bar plot of the `source_type` variable, colored by the target variable, `status_group`, in the training data can be found in Figure 5.

\vspace{1cm}

```{r, message=F, warning=F, fig.height=.5, echo = F, fig.align="center", fig.width = 9}
ggplot() + ggtitle("Table 2", subtitle = "Unique Values in each of the Variables that Provide Water Source Information                     ") + theme_minimal()
```
\begin{singlespace}
\begin{center}
\begin{tabular}{l l l}
  source & source\_type & source\_class \\ \hline
  `r source[1]` & `r levels(data$source_type)[1]` & `r source_class[1]`\\
  `r source[2]` & `r levels(data$source_type)[2]` & `r source_class[2]`\\
  `r source[3]` & `r levels(data$source_type)[3]` & `r source_class[3]`\\
  `r source[4]` & `r levels(data$source_type)[4]` & \\
  `r source[5]` & `r levels(data$source_type)[5]` & \\
  `r source[6]` & `r levels(data$source_type)[6]` & \\
  `r source[7]` & `r levels(data$source_type)[7]` & \\
  `r source[8]` &  & \\
  `r source[9]` &  & \\
  `r source[10]` &  & \\
\hline
\end{tabular}
\end{center}
\end{singlespace}

\vspace{2cm}

```{r, warning=F, message = F, echo = F, fig.height=4, fig.width=7}
ggplot(data = data) + geom_bar(aes(x = source_type, fill=status_group)) + 
  theme(legend.position = "right", axis.text.x=element_text(angle = -60, hjust = 0)) + 
  ggtitle("Figure 5", subtitle = "Bar Plot of the Source Type Variable Colored by Status Group") + 
  labs(x = "Source Type", y = "Count", fill = "Status Group")
```

\vspace{1cm}

$\quad$ There are variables in the data set that appear to be manually maintained and have too many unique values to be managable. These variables were eliminated as well as any variables with only one unique value. The `population` and `construction_year` variables were removed because they were used to create the split between training and testing data sets. All variables, except `basin`, providing information on the location of the water point were eliminated and few other variables were excluded for lack of predictive power. Basin was kept because it gives information on the geographic location of the water points with regard to the water basin they pull from. The data began with 40 feature columns (excluding the target variable, `status_group`). After feature selection, the resulting data set has 13 feature columns and the target variable. Table 3 is a compilation of the final 13 features, their descriptions, and values that each can take.   



\vspace{1cm}   


```{r, message=F, warning=F, fig.height=.5, echo = F, fig.align="center", fig.width = 7}
ggplot() + ggtitle("Table 3", subtitle = "Final Set of Predictor Variables                                                                                                                             ") + theme_minimal()
```
\begin{singlespace}
\begin{center}
\footnotesize
\begin{tabular}{l l p{7cm}}
  Variable Name & Values & Description  \\ \hline
  amount\_tsh & Numeric, (0, 200000) & Total static head amount (amount of water available to the water point)\\
  gps\_height & Numeric, (-90.0, 2232.0) & Altitude of the well \\ 
  basin & Factor, 9 levels & Geographic water basin the water point draws from \\
  public\_meeting & Factor, True/False/Unknown & Was there a public meeting held? \\
  scheme\_management & Factor, 11 levels & Who operates the water point \\
  permit & Factor, True/False/Unknown & If the water point is permitted \\
  extraction\_type\_class & Factor, 7 levels & The type of extraction the water point uses \\
  management\_group & Factor, 5 levels & The type of entity that manages the water point \\
  payment & Factor, 7 levels & When do users have to pay to use the water point?  \\
  quality\_group & Factor, 6 levels & A measure of water quality at the water point  \\
  quantity & Factor, 5 levels & A measure of whether the quantity of water available from the water point is sufficient \\
  source\_type & Factor, 7 levels & The type of source the water comes from \\
  waterpoint\_type & Factor, 7 levels & The type of water point used \\
\hline
\end{tabular}
\end{center}
\end{singlespace}
\vspace{1cm}



## Analysis
Four prediction methods are detailed in the following portion of the paper. Each model was built on the "cornered" training data set and used to predict water points in the testing set.


### Logistic Regression
Logistic regression has a long history in the field of statistics and is one of the most widely used source of classifiers [@james_witten_hastie_tibshirani]. In general, it is a method of estimating conditional probabilities as a function of explanatory variables and regression coefficients, and is often used to classify binary response data which is how we will apply it here. A logistic regression model was fit to the training data using the `glm` function in `R`. For logistic regression, categorical variables (stored as `factors` in `R`) are expanded into binary indicator variables. The full equation can be found in the appendix.  Using the model to classify each water pump in the testing set yields the confusion matrix and statistics in Table 4. The largest advantages of implementing a logistic regression model are interpretability and computational convenience. Since we are extrapolating, the relatively low values of Accuracy, Sensitivity, and Specificity are not surprising. Kuhn and Johnson (2013) write that Kappa values within 0.30 to 0.50 indicate reasonable agreement, so although it is on the low end, we are not so dissatisfied with the Kappa statistic of 0.3703.  

```{r, echo = F, message = F, warning = F}
## LOGISTIC REGRESSION
set.seed(1)
model_logistic_regression_13 <- glm(status_group ~.,family=binomial(link='logit'),data=train_13)
model_logistic_regression_15 <- glm(status_group ~.,family=binomial(link='logit'),data=train_15)

#summary(model_logistic_regression)
a_13 <- anova(model_logistic_regression_13, test = "Chisq")
a_15 <- anova(model_logistic_regression_15, test = "Chisq")

#LRT full vs. reduced model
a_comp <- anova(model_logistic_regression_13, model_logistic_regression_15, test = "Chisq")
```


```{r, echo = F, message = F, warning = F}
## LOGISTIC REGRESSION

### Cross-Validation
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3, savePredictions = TRUE)

model_logistic_regression_15_CV <- train(status_group~.,  data=train_15, method="glm", family="binomial", trControl = ctrl)
model_logistic_regression_13_CV <- train(status_group~.,  data=train_13, method="glm", family="binomial", trControl = ctrl)

# Cross validation accuracy and Kappa are in
# model_logistic_regression_15_CV$results, and
# model_logistic_regression_13_CV$results

# predict on test data
preds_logistic_regression_15_CV <- predict(model_logistic_regression_15_CV, newdata = test_15)
preds_logistic_regression_13_CV <- predict(model_logistic_regression_13_CV, newdata = test_13)

cm_15 <- confusionMatrix(data=preds_logistic_regression_15_CV, test_y)
cm_13 <- confusionMatrix(data=preds_logistic_regression_13_CV, test_y)


#preds_logistic_regression <- predict(model_logistic_regression, newdata = test_logistic_regression) #predict 
#preds_logistic_regression_factor <- ifelse(preds_logistic_regression < 0, "functional", "requires attention") #convert numeric predictions to factor to match response variable
#confusionMatrix(preds_logistic_regression_factor, test_y)
```



\vspace{1cm}


```{r, message=F, warning=F, fig.height=.5, echo = F, fig.align="center", fig.width = 7}
ggplot() + ggtitle("Table 4", subtitle = "Confusion Matrix and Statistics for Classification by Logistic Regression with 13 predictors                                          ") + theme_minimal()
```

\begin{center}
\footnotesize
\begin{tabular}{l r c c}
    & & \multicolumn{2}{ c }{Reference} \\
    & & functional & requires attention \\ \cline{3-4} 
    Prediction & \multicolumn{1}{r|}{functional} & `r cm_13$table[1,1]` & `r cm_13$table[1,2]` \\
    & \multicolumn{1}{r|}{requires attention} & `r cm_13$table[2,1]` & `r cm_13$table[2,2]` \\
\end{tabular}
\begin{tabular}{llrl}
    &&& \\
    &    &   Kappa :&`r round(cm_13$overall[2], 4)`\\
    &    &   Accuracy :&`r round(cm_13$overall[1], 4)`\\
    &    &   Sensitivity :&`r round(cm_13$byClass[1], 4)` \\               
    &    &   Specificity :&`r round(cm_13$byClass[2], 4)`\\
\end{tabular}
\end{center}
\vspace{1cm}


```{r, message=F, warning=F, fig.height=.5, echo = F, fig.align="center", fig.width = 7}
ggplot() + ggtitle("Table 5", subtitle = "Confusion Matrix and Statistics for Classification by Logistic Regression with 15 predictors                                           ") + theme_minimal()
```

\begin{center}
\footnotesize
\begin{tabular}{l r c c}
    & & \multicolumn{2}{ c }{Reference} \\
    & & functional & requires attention \\ \cline{3-4} 
    Prediction & \multicolumn{1}{r|}{functional} & `r cm_15$table[1,1]` & `r cm_15$table[1,2]` \\
    & \multicolumn{1}{r|}{requires attention} & `r cm_15$table[2,1]` & `r cm_15$table[2,2]` \\
\end{tabular}
\begin{tabular}{llrl}
    &&& \\
    &    &   Kappa : &`r round(cm_15$overall[2], 4)`\\
    &    &   Accuracy :&`r round(cm_15$overall[1], 4)`\\
    &    &   Sensitivity :&`r round(cm_15$byClass[1], 4)` \\               
    &    &   Specificity :&`r round(cm_15$byClass[2], 4)`\\
\end{tabular}
\end{center}
\vspace{1cm}



### Support Vector Machine (SVM)


$\quad$ \textbf{S}upport \textbf{V}ector \textbf{M}achines (SVMs) are a more flexible procedure that enlarges the featrue space using basis expansions. Linear classifiers generally achieve better training class separation in this enlarged space. These classifiers translate to nonlinear classification boundaries in the original space. With SVMs the dimension of the enlarged feature space is allowed to get very large [@hastie_tibshirani_friedman]. 

$\quad$ An SVM model with a _radial_ kernal and a cost parameter of _50_ was fit with the `e1071` package in `R`. The cost paramater was tuned using repeated 10-fold cross validation with the `tune` function in the `e1071` package in `R`. The confusion matrix and prediction statistics are included in Table 5. We see here that classification using a an SVM model yields slightly lower values of accuracy and the Kappa statistic, but still keeps a Kappa value above 0.3.


```{r, echo = F, message = F, warning = F, eval = F}
## SVM
set.seed(1)

# tune
svm_tune <- tune(svm, status_group~.,
                 data = train_logistic_regression,
                 kernel="radial",
                 tunecontrol = tune.control(nrepeat = 3, cross = 10),
                 ranges=list(cost=c(30, 40, 50 , 60)))

print(svm_tune)

model_svm <- svm(status_group ~ ., data=train_logistic_regression, kernel = "radial", cost = 50)

summary(model_svm)

preds_svm <- predict(model_svm, newdata = test_logistic_regression) #predict
confusionMatrix(preds_svm, test_y)
```

```{r, message=F, warning=F, fig.height=.5, echo = F, fig.align="center", fig.width = 7}
ggplot() + ggtitle("Table 5", subtitle = "Confusion Matrix and Statistics for Classifaction by Support Vector Machine (SVM)                                           ") + theme_minimal()
```

\vspace{1cm}
\begin{center}
\footnotesize
\begin{tabular}{l r c c}
    & & \multicolumn{2}{ c }{Reference} \\
    & & functional & requires attention \\ \cline{3-4} 
    Prediction & \multicolumn{1}{r|}{functional} & 12689 & 4084 \\
    & \multicolumn{1}{r|}{requires attention} & 7081 & 10604 \\
\end{tabular}
\begin{tabular}{llrl}
    &&& \\
    &    &  Kappa: &0.3545\\
    &    &  Accuracy :&0.676 \\
    &    &  Sensitivity :&0.6418 \\               
    &    &  Specificity :&0.7219\\
\end{tabular}
\end{center}
\vspace{1cm}


### Random Forest
$\quad$ Random forest is a tree-based method that can be used for classification. This algorithm builds a number of decision trees on bootstrapped training samples. During building, at each node (or split) of each tree, radomly select a subset of the input features and find an optimal single split. For each tree, repeat splitting in this fashion up to a fixed depth or until no single split improvement is possible without creating a split resulting in a small number of cases. Many trees are built in this fashion, then aggregated [@james_witten_hastie_tibshirani]

$\quad$ A random forest model was trained using the `randomForest` function from the `randomForest` package in `R`. The parameter `mtry` determines the number of features randomly sampled as candidates at each split. An `mtry` value of 6 was tuned using `tuneRF`, a function that is also from the `randomForest` package which searches to the left and right of the default `mtry` value (which in this case is 3) to minimize out of bag (OOB) error. The resulting confusion matrix and statistics are in Table 6. We see that both logistic regression and SVMs outperform the random forest according to the Accuracy and Kappa values. Additionally, the sensitivity value of 0.5945 is notably low.

```{r, warning=F, message = F, echo = F, fig.height=4, fig.width=7, eval = F}
rf_tune <- tuneRF(x = train_logistic_regression[,2:13], y = train_logistic_regression[,1], ntreeTry = 500, plot = F)
 
 
ggplot(data = as.data.frame(rf_tune)) + geom_point(aes(x = mtry, y = OOBError)) + 
  ggtitle("Figure 6", subtitle = "Tune Parameter mtry for Random Forest") + 
  scale_x_continuous(breaks = 2:12) + 
  labs(y = "OOB Error")
```




```{r, warning = F, message = F, echo = F, eval = F}
set.seed(1)


rf_tune <- tuneRF(x = train_logistic_regression[,2:13], y = train_logistic_regression[,1], ntreeTry = 500)

model_forest <- randomForest(status_group ~ ., data = train_logistic_regression, importance = T, ntree = 1500, mtry = 6)

# Use random forest to predict the values in test
preds_forest <- predict(model_forest, test_logistic_regression)
confusionMatrix(preds_forest, test_y)

importance(model_forest)
```
\vspace{1cm} 


```{r, message=F, warning=F, fig.height=.5, echo = F, fig.align="center", fig.width = 7}
ggplot() + ggtitle("Table 6", subtitle = "Confusion Matrix and Statistics for Classifaction by Random Forest                                           ") + theme_minimal()
```
\begin{center}
\footnotesize
\begin{tabular}{l r c c}
    & & \multicolumn{2}{ c }{Reference} \\
    & & functional & requires attention \\ \cline{3-4} 
    Prediction & \multicolumn{1}{r|}{functional} & 11771 & 3799 \\
    & \multicolumn{1}{r|}{requires attention} & 7999 & 10889 \\
\end{tabular}
\begin{tabular}{llrl}
    &&& \\
    &    &   Kappa: &0.3248\\
    &    &   Accuracy :&0.6576\\
    &    &   Sensitivity :&0.5954 \\               
    &    &   Specificity :&0.7414\\
\end{tabular}
\end{center}

\vspace{1cm}

### Active Set Selection Classification (ASSM)

$\quad$ Dataset shift is a common problem in predictive analytics that presents itself in many practical applications. This phenomenon occurs when the joint distribution of inputs and outputs differs between training and testing stages of predictive modeling. It may be present for a variety of reasons such as the inability to reproduce testing conditions during training, or because experimental design introduces bias. In the real world, it is often the case that the conditions under which we use the models that we develop will differ from the conditions in which the model was developed. Environments are generally nonstationary and while "textbook" predictive machine learning algorithms are powerful, they generally ignore these differences. [@dataset_shift]   

$\quad$ \textbf{A}ctive \textbf{S}et \textbf{S}election \textbf{C}lassification (ASSM) is the next classification method we will consider. ASSM is introduced by Wen Zhou in Chapter 4 of his doctoral dissertation [@rick]. The chapter discusses the covariate shift problem, which is a particular type of dataset shift where the distribution of feature vectors are possibly different between training and testing sets. By training existing classifiers using subsets of training data that are similar (in some sense) to the training cases, one is able to mitigate a large amount of the effect brought on by the covariate shift problem. ASSM is felixible and can be used with existing methods of classification. Zhou details the ASSM algorithm in Algorithm 4.2.2 of his paper. Here we implement ASSC with an SVM classifier (ASSM-SVM) in the same fashion, substituting our own functions and parameters in the algorithm as follows.  
 

$\quad$ First, lay the notational groundwork used in Algorithm 4.2.2 of Zhou (2014). Consider the training set $\mathcal{T}$ with $n = 2,886$ (labeled) observations $\{(Y_i, \mathbf{X}_i) \}_{i = 1}^n$ where $Y_i$ are the corresponding class labels and $\mathbf{X}_i$ are $p$-dimensional feature vectors. Recall that for the well data, $p=13$. Also consider the testing (or predicting) set $\mathcal{P}$ with $m$ (unlabeled) observations $\{\mathbf{U}_j \}_{j = 1}^m$ which are $p$-dimensional feature vectors to be classified. The algorithm is very computationally expensive, so for demonstrative purposes I will simply consider a random subset of size $m = 300$ from $\mathcal{T}$.  


Step 1.   



> Compute an __association matrix__ $W_{m \times n} := [w_{ji}]_{j=1;i=1}^{m,n}$ that for $X_i=(X_{i1}, \hdots, X_{ip})'\in \mathcal{T}$ and  $U_j=(U_{j1}, \hdots, U_{jp})'\in \mathcal{P}$, 

$$  
w_{ji}=\mbox{exp}\left \{ - \ddfrac{\sum_{k=1}^{p}d_k^2(U_{jk}, X_{ik})}{\lambda}   \right \},
$$
 
 
 
> where $\lambda$ can be thought of as a tuning parameter, and $d_k(u,x)$ denotes a distance (or dissimilarity) function of $u$ and $x$ on the $k^{th}$ coordinate of the feature vector. That is, for standardized numeric features `amount_tsh` and `pgs_height` ($k = 1,2$) use Euclidean distance $$d_{k=1,2}(u,x) = \sqrt{(u_{jk} - x_{ik})^2}.$$ 
   
> All other features ($k = 3, \hdots, 13$) are categorical. Use the following weighted degree-of-difference measure to assess dissimilarity for these features. __reference5__ (Hastie, Tibshirani, Friedman)   
\[ d_{k = 3, \hdots, 13}(u,x) = \tau \mbox{ I}[u_{jk} \ne x_{ik}] = \tau \left\{
\begin{array}{ll}
      1 & u_{jk} \ne x_{ik} \\
      0 & u_{jk} = x_{ik} \\
\end{array} 
\right., \]
where $\tau$ is the weight we will give to a pair of dissimilar categorical features at every coordinate $k = 3, \hdots, 13$.
Thus, we compute each $[w_{ji}]_{j=1;i=1}^{m,n}$ as  
$$  
w_{ji}=\mbox{exp}\left \{ - \ddfrac{\sum_{k=1}^{2}(u_{jk} - x_{ik})^2 + \tau^2\sum_{k=3}^{13}\mbox{ I}[u_{jk} \ne x_{ik}]}{\lambda}   \right \}.
$$

   
   
Step 2.  


> Compute the __selection probability matrix__ $P_{m \times n} := [p_{ji}]_{j=1;i=1}^{m,n}$ with
\[
p_{ji} = \frac{w_{ji}}{\sum_{i = 1}^{n} w_{ji}}.
\]



Step 3.  

> Fix B $>0$, and for the $b^{th}$ iteration where $b=1,\hdots,B$, do the following.   

$\,$ (a.)   

\vspace{-10.8mm}
> For each $U_j \in \mathcal{P}(j = 1, \hdots, m)$, sample $q<n$ observations from $\mathcal{T}$ with replacement based on the selection probability that
\[
\left(Y_{j,l}^b, X_{j,l}^b\right) := (Y_i^*, X_i^*)|\mathcal{T}\stackrel{iid}{\sim}\{p_{ji}\}_{i=1}^{n}
\]
> for $l = 1, \hdots, q$ ($X^*$ denotes a sampling version of $X$), so that we obtain a __selected active set__ (i.e. a $b^{th}$ training set of size $mq$ for predictions on $\mathcal{P}$)
\[  
\mathcal{S}_b^q = \left \{ \left(Y_{j,l}^b, X_{j,l}^b \right) \right\}_{j=1,l=1}^{m,q}.
\]

$\,$ (b.)   

\vspace{-10.8mm}  
>  Train an SVM classifier with parameter cost $= 10$ on the selected active set $\mathcal{S}_b^q$ by which we make predictions $\hat{V}_{j,b}$ for observations in $\mathcal{P}$.

\vspace{5mm}

Step 4.

> Obtain the predictions for observations in $\mathcal{P}$ by the majority voting that   
\[  
\hat{V}_j^{ASSC} = \mbox{arg max}\sum_{b=1}^{B} I\left[\hat{V}_{j,b} = c \right]
\]

> for $j = 1, \hdots, m$. The collection $\mathcal{S}_B(m,n,q) = \cup_{b=1}^{B}\mathcal{S}_b^q$ is called the __active set__ for $\mathcal{T}$ with respect to $\mathcal{P}$.



$\quad$ Implementing this algorithm using a random sample of size $m = 200$ from $\mathcal{P}$ with parameters $B = 201$, $q = 100$, $\tau = 1.5$, $\lambda = 5$ yields the confusion matrix and statistics in Table 7. Because a subset of the testing cases were used the reference distribution of `functional` and `requires attention` wells will likely differ from the overall distribution for the entire set $\mathcal{P}$ (i.e. the no information rates will differ). Because the Kappa statistic takes into account the accuracy that would be generated simply by chance [@kuhn_johnson] it can be used to compare accross the models presented here. Notice that the Kappa statistic of 0.3624 is higher than that of SVM and random forest. Thus, by fitting an SVM classifier to $B = 201$ weighted bootstrap samples of (only) size $q = 100$ and letting each classifier vote to make a final prediction for each test case, we are able to outperform both SVM and random forst which were both optimally tuned. Particularly interesting is that the ASSC-SVM outperformed the optimally tuned SVM which implies that ASSC does alter the decision boundary based on the predicting set [@rick].

```{r, echo = FALSE, eval = F}
## STEP 0
 # create small workable data sets
 test_13_target <- cbind(test_13, test_y)
 test_13_small <- test_13_target[sample(nrow(test_13_target), 300),]
 train_13_small <- train_13
```  
  
  
```{r, echo = FALSE, eval = F}
 ### STEP 1
 # dissimilarity/distance function
 tau <- 1.5  # set weight for categorical variable distance peice of function
 lambda <- 3
 
 d_13 <- function(u, x){
   # categorical variables (11) <-  # of variables that don't match
   # continuous variables (2) <- euclidean distance
   # combine linearly and give a wieght to the categorical variable distance so that it has a reasonable amount of say in the distance between the two cases being compared
   dissimilarity <- sum((u[1:2]-x[1:2])^2) + tau^2*sum(u[3:13] != x[3:13])
 }
  
 # w is the "association matrix" higher values in this matrix indicate a higher association between the train and test cases being compared.
 w <- matrix(nrow = nrow(test_13_small), ncol = nrow(train_13_small))
 for(j in 1:nrow(test_13_small)){
   for(i in 1:nrow(train_13_small)){
     w[j,i] <- exp(-d_13(subset(test_13_small[j,], select = -c(test_y)), subset(train_13_small, select = -c(status_group))[i,])/lambda)
   }
 } 
beep(2)
```

 
 
```{r, echo = FALSE, eval = F}
 ### STEP 2
 # selection probability matrix
 p <- matrix(nrow = nrow(test_13_small), ncol = nrow(train_13_small))
 for(j in 1:nrow(test_13_small)){
   p[j,] <- w[j,]/apply(w, 1, sum)[j]
 } 
```
 
 
```{r, echo = FALSE, eval = F}
 ### STEP 3
 
 B <- 201  # number of samples to take and vote 
 predictions <- matrix(nrow = nrow(test_13_small), ncol = B)
 
 for(b in 1:B){
     
   for(j in 1:nrow(test_13_small)){  
     
     q <- 150  #size of weighted bootstrap sample
     active_set_b <- c()
     for(l in 1:q){
       # set presets for flag, index, and cumulative probability
       flag <- 0
       index <- 0
       cum_prob <- 0
       
       # simulate a uniform(0,1) rv
       unif <- runif(1)
       
       while(flag == 0){
         index <- index + 1
         cum_prob <- cum_prob + p[j,index]
         
         if(unif <= cum_prob){
           flag <- 1
         }
       }
       
       
       active_set_b <- rbind(active_set_b, train_13_small[index,])
       
     }
     
     # if the numeric variables are constant wiggle it a little so the classifier doesn't break
     if(sd(active_set_b[,1]) == 0){active_set_b[1,1] <- active_set_b[1,1] + 0.001}
     if(sd(active_set_b[,2]) == 0){active_set_b[1,2] <- active_set_b[1,2] + 0.001}
     
 
     
     #### BUILD CLASSIFIER 
     # SVM
     if(length(unique(active_set_b$status_group)) == 1){
       predictions[j,b] <- active_set_b$status_group[1]  
     }else{
       model_svm_b <- svm(status_group ~ ., data = active_set_b, kernel="radial", cost = 10)
       predictions[j, b] <- ifelse(predict(object = model_svm_b, newdata = subset(test_13_small[j,], select = -c(test_y))) == "functional", "functional", "requires attention")
     }
   }
 } 
 
beep(2)
```

```{r, echo = FALSE, eval = F}
 voting <- function(x){
   sum(x == "functional")
 }
 
 preds_assc <- ifelse(apply(predictions, 1, voting) >= (B+1)/2, "functional", "requires attention")
 confusionMatrix(preds_assc, test_13_small$test_y)
```

\vspace{1cm} 


```{r, message=F, warning=F, fig.height=.5, echo = F, fig.align="center", fig.width = 7}
# library(ggplot2)
ggplot() + ggtitle("Table 7", subtitle = "Confusion Matrix and Statistics for Classifaction by ASSCSVM                                           ") + theme_minimal()
```
\begin{center}
\footnotesize
\begin{tabular}{l r c c}
    & & \multicolumn{2}{ c }{Reference} \\
    & & functional & requires attention \\ \cline{3-4} 
    Prediction & \multicolumn{1}{r|}{functional} & 96 & 37 \\
    & \multicolumn{1}{r|}{requires attention} & 59 & 108 \\
\end{tabular}
\begin{tabular}{llrl}
    &&& \\
    &    &   Kappa :&0.3624\\
    &    &   Accuracy :&0.68\\
    &    &   Sensitivity :&0.6194 \\               
    &    &   Specificity :&0.7448\\
\end{tabular}
\end{center}

\vspace{2cm}




## Discussion  
$\quad$ 

\vspace{2cm}


## Future Work  
$\quad$   


  
\pagebreak
## Appendix  
  
#### Logistic Regression Equation    
\begin{singlespace}
\small
$$
\begin{aligned}
logit(&status\_group) =  \beta_0 + \beta_{1,1}x_1 + \beta_{2,1}x_2 \\ 
& + \beta_{3,1}I_{\{x_3 = Lake Nyasa\}} + \beta_{3,2}I_{\{x_3 = Lake Rukwa\}} \\ &  + \beta_{3,3}I_{\{x_3 = Lake Tanganyika\}} + \beta_{3,4}I_{\{x_3 = Lake Victoria\}} + \beta_{3,5}I_{\{x_3 = Pangani\}} \\ &+ \beta_{3,6}I_{\{x_3 = Rufiji\}} + \beta_{3,7}I_{\{x_3 = Ruvuma / S. Coast\}}+ \beta_{3,8}I_{\{x_3 = Wami / Ruvu\}} \\ 
& + \beta_{4,1}I_{\{x_4 = True\}} + \beta_{4,2}I_{\{x_4 = Unknown\}} \\ 
&+ \beta_{5,1}I_{\{x_5 = Other\}} + \beta_{5,2}I_{\{x_5 = Parastatal\}} + \beta_{5,3}I_{\{x_5 = Private Op.\}} + \beta_{5,4}I_{\{x_5 = SWC\}} \\ &+ \beta_{5,5}I_{\{x_5 = Trust\}} + \beta_{5,6}I_{\{x_5 = VWC\}} + \beta_{5,7}I_{\{x_5 = Water Auth.\}} \\ &+ \beta_{5,8}I_{\{x_5 = Water Board\}}+ \beta_{5,9}I_{\{x_5 = WUA\}}+ \beta_{5,10}I_{\{x_5 = WUG\}} \\
& + \beta_{6,1}I_{\{x_6 = True\}} + \beta_{6,2}I_{\{x_6 = Unknown\}} \\ 
& + \beta_{7,1}I_{\{x_7 = handpump\}} + \beta_{7,2}I_{\{x_7 = motorpump\}} +\beta_{7,3}I_{\{x_7 = other\}} \\& + \beta_{7,4}I_{\{x_7 = submersible\}}+ \beta_{7,5}I_{\{x_7 = wind-powered\}} \\
& + \beta_{8,1}I_{\{x_8 = other\}} + \beta_{8,2}I_{\{x_8 = parastatal\}} + \beta_{8,3}I_{\{x_8 = unkown\}} + \beta_{8,4}I_{\{x_8 = user-group\}} \\ 
& + \beta_{9,1}I_{\{x_9 = other\}} + \beta_{9,2}I_{\{x_9 = pay annually\}}+ \beta_{9,3}I_{\{x_9 = pay monthly\}} \\& + \beta_{9,4}I_{\{x_9 = pay per bucket\}} + \beta_{9,5}I_{\{x_9 = pay when scheme fails\}} + \beta_{9,6}I_{\{x_9 = unknown\}} \\
& + \beta_{10,1}I_{\{x_{10} = flouride\}} + \beta_{10,2}I_{\{x_{10} = good\}} + \beta_{10,3}I_{\{x_{10} = milky\}}+ \beta_{10,4}I_{\{x_{10} = salty\}} + \beta_{10,5}I_{\{x_{10} = unkown\}} \\
& + \beta_{11,1}I_{\{x_{11} = enough\}}+ \beta_{11,2}I_{\{x_{11} = insufficient\}}+ \beta_{11,3}I_{\{x_{11} = seasonal\}} + \beta_{11,4}I_{\{x_{11} = unkown\}} \\ 
&  + \beta{12,1}I_{\{x_{12} = surface\}}  + \beta{12,1}I_{\{x_{12} = unknown\}} \\
& + \beta_{13,1}I_{\{x_{13} = communal standpipe\}}+ \beta_{13,2}I_{\{x_{13} = communal standpipe multiple\}} + \beta_{13,3}I_{\{x_{13} = hand pipe\}} \\ &+ \beta_{13,4}I_{\{x_{13} = hand pipe\}} + + \beta_{13,5}I_{\{x_{13} = improved spring\}} + \beta_{13,1}I_{\{x_{13} = other\}}
\end{aligned}
$$

where,  \newline
$x_1$ = amount\_tsh  \newline
$x_2$ = gps\_height  \newline
$x_3$ = basin  \newline
$x_4$ = public\_meeting  \newline
$x_5$ = scheme\_management  \newline
$x_6$ = permit  \newline
$x_7$ = extraction\_type\_class  \newline
$x_8$ = management\_group  \newline
$x_9$ = payment  \newline
$x_{10}$ = quality\_group  \newline
$x_{11}$ = quantity  \newline
$x_{12}$ = source\_class  \newline
$x_{13}$ = waterpoint\_type  \newline

\end{singlespace}

\pagebreak  
## References  


